// // law_as_code_chatbot/src/lib/graph/nodes/generate.ts
// import { InputStateAnnotation } from "../state";
// import { StateAnnotation } from "../state";
// import { llm } from "../../llm/model";
// import { ragPrompt } from "../../llm/prompts";
// import { streamText } from "ai";
// import { openai } from "@ai-sdk/openai";

// // // non-streaming version
// // export async function generate(state: typeof StateAnnotation.State) {
// //   const docsContent = (state.context ?? []).map(d => d.pageContent).join("\n");
// //   const messages = await ragPrompt.invoke({
// //     question: state.question,
// //     context: docsContent,
// //   });
// //   const response = await llm.invoke(messages);
// //   return { answer: String(response.content ?? "") };
// // }

// export async function generate(state: typeof StateAnnotation.State) {
//   const contextWithRefs = (state.context ?? []).map((d, i) => {
//   const ref = `[${i + 1}] ${d.metadata.title} (${d.metadata.url})`;
//   return `${ref}\n${d.pageContent}`;
// }).join("\n\n");
//   const references = (state.context ?? []).map(d => ({
//   title: d.metadata.title,
//   url: d.metadata.url,
//   id: d.metadata.id,
// }));
//   const messages = await ragPrompt.invoke({
//     question: state.question,
//     context: contextWithRefs,
//     references,
//   });
//   const response = await llm.invoke(messages.toChatMessages() as any);

//   return {
//     answer: String(response.content ?? ""),
//     references
//   };
// }


// // // streaming version
// // export async function* generateStream(state: {
// //   question: string;
// //   context: Array<{ pageContent: string}>;
// // }) {
// //   const docsContent = (state.context ?? []).map(d => d.pageContent).join("\n");
// //   const messages = await ragPrompt.invoke({
// //     question: state.question,
// //     context: docsContent,
// //   });
// //   const prompt = String(await ragPrompt.invoke({
// //   question: state.question,
// //   context: docsContent,
// // }));
// //   const {textStream} = streamText({
// //     model: openai("gpt-4o"),
// //     prompt
// // });
// //   for await (const textPart of textStream) {
// //     yield textPart;
// //   }
// // }

// // streaming version
// export async function* generateStream({
//   question,
//   context,
//   references,
// }: {
//   question: string;
//   context: string;
//   references?: Array<{ title: string; url?: string | null }>;
// }) {
//   // Build a context section that includes references visibly numbered
//   const contextWithRefs = context + "\n\nReferences:\n" + 
//     (references?.map((r, i) => `[${i + 1}] ${r.title} (${r.url})`).join("\n") ?? "");
//   // const contextWithRefs = (state.context ?? [])
//   // .map((d, i) => `[${i + 1}] ${d.metadata.title} (${d.metadata.url})\n${d.pageContent}`)
//   // .join("\n\n");

//   const messages = await ragPrompt.invoke({
//     question,
//     // context
//     context: contextWithRefs,
//   });
//   const prompt = String(messages)

//   // Stream from OpenAI
//   const { textStream } = streamText({
//     model: openai("gpt-4o"),
//     prompt,
//   });

//   // Yield tokens as they arrive
//   for await (const textPart of textStream) {
//     yield textPart;
//   }
// }



// law_as_code_chatbot/src/lib/graph/nodes/generate.ts
import { StateAnnotation } from "../state";
import { llm } from "../../llm/model";
import { ragPrompt } from "../../llm/prompts";
import { streamText } from "ai";
import { openai } from "@ai-sdk/openai";

// å¼•å…¥æ•°æ®åº“å’Œ Auth
import { getServerSession } from "next-auth";
import { authOptions } from "../../../app/api/auth/[...nextauth]/route";
import connectDB from "../../databse_user/db";
import { User } from "../../databse_user/user";

// --- Helper: è·å–å½“å‰ç”¨æˆ·çš„ç”»åƒ ---
async function getUserPersona() {
  try {
    const session = await getServerSession(authOptions);
    if (!session || !session.user?.email) {
      return "A general user interested in Minnesota law."; 
    }
    await connectDB();
    const user = await User.findOne({ email: session.user.email });
    return user?.legalPersona || "A new user interested in Minnesota law.";
  } catch (error) {
    console.error("Failed to fetch user persona:", error);
    return "A general user interested in Minnesota law.";
  }
}

// --- Non-streaming version ---
export async function generate(state: typeof StateAnnotation.State) {
  const userProfile = await getUserPersona();
  console.log(`ğŸ‘¤ Using Persona: "${userProfile}"`);

  // ç¡®ä¿ context æ˜¯å­—ç¬¦ä¸²
  const contextContent = Array.isArray(state.context)
    ? state.context.map((d: any) => d.pageContent).join("\n\n")
    : (state.context as string);

  const promptValue = await ragPrompt.invoke({
    question: state.question,
    context: contextContent,
    user_profile: userProfile,
  });

  // ğŸ”¥ ä¿®å¤ç‚¹ 1: å¿…é¡»è°ƒç”¨ .toChatMessages() æ‰èƒ½ä¼ ç»™ llm.invoke
  const response = await llm.invoke(promptValue.toChatMessages());

  return {
    answer: String(response.content ?? ""),
    // ğŸ”¥ ä¿®å¤ç‚¹ 2: ä½¿ç”¨ (state as any) ç»•è¿‡ç±»å‹æ£€æŸ¥ï¼Œå› ä¸º state å®šä¹‰é‡Œå¯èƒ½æ¼äº† references
    references: (state as any).references 
  };
}

// --- Streaming version ---
export async function* generateStream({
  question,
  context,
  references,
}: {
  question: string;
  context: string;
  references?: Array<{ title: string; url?: string | null }>;
}) {
  const userProfile = await getUserPersona();
  
  // å‡†å¤‡ Prompt
  const promptValue = await ragPrompt.invoke({
    question,
    context: context, 
    user_profile: userProfile,
  });

  // æŠŠ LangChain çš„ messages è½¬æ¢æˆ Vercel AI SDK èƒ½æ‡‚çš„æ ¼å¼ (String æ¯”è¾ƒé€šç”¨)
  // è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ .toString() è·å–ç®€å•çš„æ–‡æœ¬è¡¨ç¤ºï¼Œæˆ–è€… .format()
  // æ³¨æ„ï¼šå¦‚æœ ragPrompt æ˜¯ ChatPromptTemplateï¼Œç›´æ¥ toString å¯èƒ½å¾—åˆ°å¯¹è±¡å­—ç¬¦ä¸²
  // æ›´ç¨³å¦¥çš„æ–¹æ³•æ˜¯å†æ¬¡ invoke å¹¶è½¬ stringï¼Œæˆ–è€…æ‰‹åŠ¨æ‹¼æ¥ã€‚
  // ä¸ºäº†ç®€å•èµ·è§ï¼Œä¸”ç¡®ä¿ Vercel SDK æ­£å¸¸å·¥ä½œï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥æ„é€  prompt string
  const messages = await promptValue.toChatMessages();
  
  // å°† system message å’Œ human message æ‹¼æˆä¸€ä¸ªå¤§å­—ç¬¦ä¸²ç»™ streamText (å…¼å®¹æ€§æœ€å¥½)
  const finalPrompt = messages.map(m => `${m._getType().toUpperCase()}: ${m.content}`).join("\n\n");

  const { textStream } = streamText({
    model: openai("gpt-4o"), 
    prompt: finalPrompt,   
  });

  for await (const textPart of textStream) {
    yield textPart;
  }
}